{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8917906,"sourceType":"datasetVersion","datasetId":5363221}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/usmanshafeeqdit/greengrowth-analyzer?scriptVersionId=187612683\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-10T05:53:39.847713Z","iopub.execute_input":"2024-07-10T05:53:39.849519Z","iopub.status.idle":"2024-07-10T05:53:39.871627Z","shell.execute_reply.started":"2024-07-10T05:53:39.849451Z","shell.execute_reply":"2024-07-10T05:53:39.869829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Enhancing Plant Growth Analysis and Classification Using Machine Learning\n\n## Background\n\nUnderstanding plant growth patterns is crucial for optimizing agricultural practices, improving crop yields, and ensuring sustainable farming. Traditional methods of monitoring and analyzing plant growth can be labor-intensive and often lack the precision required for making data-driven decisions. The GreenGrowth Analyzer project aims to address this challenge by developing a machine learning-based system that can accurately classify and analyze plant growth data.\n\n## Objective\n\nThe primary objective of the GreenGrowth Analyzer project is to develop a robust machine learning model that can classify and predict plant growth milestones based on various environmental and cultivation factors. This will help farmers, researchers, and gardeners optimize their practices by providing actionable insights into plant growth.\n\n## Data Description\n\nThe dataset for this project includes the following columns:\n\n- **Soil_Type**: The type of soil in which the plant is grown (e.g., sandy, loamy, clay).\n- **Sunlight_Hours**: The number of hours of sunlight the plant receives per day.\n- **Water_Frequency**: The frequency of watering the plant (e.g., daily, weekly).\n- **Fertilizer_Type**: The type of fertilizer used for the plant (e.g., organic, inorganic).\n- **Temperature**: The average temperature of the environment where the plant is grown.\n- **Humidity**: The average humidity level of the environment where the plant is grown.\n- **Growth_Milestone**: The stage of plant growth (e.g., germination, vegetative, flowering, fruiting).\n\n## Scope\n\n### Data Collection and Preprocessing\n\n- Collect a comprehensive dataset covering various plant species and growth conditions.\n- Clean and preprocess the data to handle missing values, outliers, and inconsistencies.\n\n### Feature Engineering\n\n- Engineer relevant features from the existing dataset to enhance model performance.\n- Analyze feature importance to identify key factors influencing plant growth.\n\n### Model Development\n\n- Develop and train multiple machine learning models to classify plant growth milestones.\n- Perform hyperparameter tuning and model selection to achieve the best performance.\n\n### Model Evaluation\n\n- Evaluate the models using appropriate metrics such as accuracy, precision, recall, and F1 score.\n- Validate the models on a separate test dataset to ensure generalizability.\n\n### Visualization and Reporting\n\n- Create intuitive visualizations to present the classification results and insights.\n- Develop a reporting tool to generate detailed reports on plant growth analysis.\n\n### Deployment\n\n- Deploy the final model as a web or mobile application for easy accessibility.\n- Integrate the application with real-time data collection devices if possible.\n\n## Expected Outcomes\n\n- A machine learning model capable of accurately classifying plant growth milestones.\n- Actionable insights and recommendations for optimizing plant growth conditions.\n- An easy-to-use application for farmers, researchers, and gardeners to monitor and analyze plant growth.\n\n## Significance\n\nThe GreenGrowth Analyzer project will significantly contribute to precision agriculture by providing a data-driven approach to plant growth analysis. It will help improve crop yields, reduce resource wastage, and promote sustainable farming practices, ultimately benefiting both small-scale and large-scale agricultural operations.\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Markdown, display\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:57:17.292378Z","iopub.execute_input":"2024-07-10T05:57:17.292828Z","iopub.status.idle":"2024-07-10T05:57:17.838143Z","shell.execute_reply.started":"2024-07-10T05:57:17.292796Z","shell.execute_reply":"2024-07-10T05:57:17.836991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1 : Load DataSet","metadata":{}},{"cell_type":"markdown","source":"### 1.1 Load dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/plant-growth-data-classification/plant_growth_data.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:39.887655Z","iopub.execute_input":"2024-07-10T05:53:39.889247Z","iopub.status.idle":"2024-07-10T05:53:39.912139Z","shell.execute_reply.started":"2024-07-10T05:53:39.889197Z","shell.execute_reply":"2024-07-10T05:53:39.910538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Basic Information","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Display first few rows of dataset","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:39.914104Z","iopub.execute_input":"2024-07-10T05:53:39.914484Z","iopub.status.idle":"2024-07-10T05:53:39.931394Z","shell.execute_reply.started":"2024-07-10T05:53:39.914451Z","shell.execute_reply":"2024-07-10T05:53:39.929929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Display the last few rows of data set","metadata":{}},{"cell_type":"code","source":"df.tail()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:39.933424Z","iopub.execute_input":"2024-07-10T05:53:39.934423Z","iopub.status.idle":"2024-07-10T05:53:39.953858Z","shell.execute_reply.started":"2024-07-10T05:53:39.934383Z","shell.execute_reply":"2024-07-10T05:53:39.952198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Display the random few rows of dataset","metadata":{}},{"cell_type":"code","source":"df.sample()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:39.956414Z","iopub.execute_input":"2024-07-10T05:53:39.957017Z","iopub.status.idle":"2024-07-10T05:53:39.988254Z","shell.execute_reply.started":"2024-07-10T05:53:39.956946Z","shell.execute_reply":"2024-07-10T05:53:39.986217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Display the size of data in rows and columns","metadata":{}},{"cell_type":"code","source":"print(\"Number of rows in dataset: \",df.shape[0])\nprint(\"Number of columns in dataset:\",df.shape[1])","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:39.991276Z","iopub.execute_input":"2024-07-10T05:53:39.992122Z","iopub.status.idle":"2024-07-10T05:53:40.00094Z","shell.execute_reply.started":"2024-07-10T05:53:39.992073Z","shell.execute_reply":"2024-07-10T05:53:39.999138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 Display basic information about the dataset","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:40.002885Z","iopub.execute_input":"2024-07-10T05:53:40.003413Z","iopub.status.idle":"2024-07-10T05:53:40.021056Z","shell.execute_reply.started":"2024-07-10T05:53:40.003378Z","shell.execute_reply":"2024-07-10T05:53:40.019652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6 Print the columns used in dataset","metadata":{}},{"cell_type":"code","source":"column_list = df.columns.to_list()\ncolumns_md = \"\\n\".join([f\"- {col}\" for col in column_list])\ndisplay(Markdown(f\"**Columns:**\\n{columns_md}\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:40.023343Z","iopub.execute_input":"2024-07-10T05:53:40.024048Z","iopub.status.idle":"2024-07-10T05:53:40.03299Z","shell.execute_reply.started":"2024-07-10T05:53:40.024011Z","shell.execute_reply":"2024-07-10T05:53:40.031642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.7 Summary statistics","metadata":{}},{"cell_type":"code","source":"styled_df = df.describe().style.set_table_styles(\n    [{\n        'selector': 'thead th',\n        'props': [('background-color', '#4CAF50'), ('color', 'white')]\n    }]\n).set_properties(**{\n    'border': '1px solid black',\n    'padding': '10px',\n    'text-align': 'center'\n}).background_gradient(cmap='Blues')\n\nstyled_df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:40.035966Z","iopub.execute_input":"2024-07-10T05:53:40.036353Z","iopub.status.idle":"2024-07-10T05:53:40.080139Z","shell.execute_reply.started":"2024-07-10T05:53:40.036322Z","shell.execute_reply":"2024-07-10T05:53:40.078632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.8 Check for missing values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:40.082496Z","iopub.execute_input":"2024-07-10T05:53:40.082999Z","iopub.status.idle":"2024-07-10T05:53:40.095971Z","shell.execute_reply.started":"2024-07-10T05:53:40.082954Z","shell.execute_reply":"2024-07-10T05:53:40.094106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"### 3.1 check missing value in each column","metadata":{}},{"cell_type":"code","source":"# Step 3: Data clean\n# check missing value in each column\n# it seems that there no missing values in any of the  columns\n# as indicated by the count of 0 missing values for each features\n \nmissing_values = df.isnull().sum()\nprint(\"Missing values in each column:\")\nprint(missing_values)\nprint(\"\\nAs we can see, all columns have 0 missing values. and no need to clean data\")","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:40.098201Z","iopub.execute_input":"2024-07-10T05:53:40.098734Z","iopub.status.idle":"2024-07-10T05:53:40.110755Z","shell.execute_reply.started":"2024-07-10T05:53:40.098687Z","shell.execute_reply":"2024-07-10T05:53:40.109006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Univariate Analysis","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Histograms for numerical columns","metadata":{}},{"cell_type":"code","source":"df.hist(bins=30,figsize=(15,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:40.11327Z","iopub.execute_input":"2024-07-10T05:53:40.113736Z","iopub.status.idle":"2024-07-10T05:53:41.186312Z","shell.execute_reply.started":"2024-07-10T05:53:40.113694Z","shell.execute_reply":"2024-07-10T05:53:41.184831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 # Box plots for numerical columns","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nsns.boxplot(data=df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:41.190545Z","iopub.execute_input":"2024-07-10T05:53:41.19096Z","iopub.status.idle":"2024-07-10T05:53:41.448228Z","shell.execute_reply.started":"2024-07-10T05:53:41.190923Z","shell.execute_reply":"2024-07-10T05:53:41.446984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3 Count plots for categorical columns","metadata":{}},{"cell_type":"code","source":"categorical_columns = df.select_dtypes(include=['object']).columns\n\n\nnum_cols = 3\nnum_rows = (len(categorical_columns) + num_cols - 1) // num_cols\n\n\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n\n\naxes = axes.flatten()\n\n\nfor i, col in enumerate(categorical_columns):\n    sns.countplot(x=col, data=df, ax=axes[i])\n    axes[i].set_title(f'Count plot of {col}')\n\n\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:41.449621Z","iopub.execute_input":"2024-07-10T05:53:41.449988Z","iopub.status.idle":"2024-07-10T05:53:42.079127Z","shell.execute_reply.started":"2024-07-10T05:53:41.449957Z","shell.execute_reply":"2024-07-10T05:53:42.077962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Bivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Scatter plots for numerical columns","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:42.080725Z","iopub.execute_input":"2024-07-10T05:53:42.081703Z","iopub.status.idle":"2024-07-10T05:53:46.743767Z","shell.execute_reply.started":"2024-07-10T05:53:42.081658Z","shell.execute_reply":"2024-07-10T05:53:46.742039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Correlation heatmap","metadata":{}},{"cell_type":"code","source":"# Convert categorical columns to numerical using one-hot encoding\ndf_encoded = pd.get_dummies(df, columns=['Soil_Type', 'Water_Frequency', 'Fertilizer_Type'])\n\n# Ensure the target column is the last one for better heatmap visualization\ntarget = df_encoded.pop('Growth_Milestone')\ndf_encoded['Growth_Milestone'] = target\n\n# Compute the correlation matrix\ncorr_matrix = df_encoded.corr()\n\n# Set the size of the plot\nplt.figure(figsize=(12, 8))\n\n# Plot the heatmap\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n\n# Add title to the heatmap\nplt.title('Correlation Heatmap of Plant Growth Data')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:46.745271Z","iopub.execute_input":"2024-07-10T05:53:46.745646Z","iopub.status.idle":"2024-07-10T05:53:47.664216Z","shell.execute_reply.started":"2024-07-10T05:53:46.745614Z","shell.execute_reply":"2024-07-10T05:53:47.66299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Multivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"### 6.1 Pair plot with hue for a categorical variable","metadata":{}},{"cell_type":"code","source":"df_encoded = pd.get_dummies(df, columns=['Soil_Type', 'Water_Frequency', 'Fertilizer_Type'])\nsns.pairplot(df, hue='Soil_Type')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:47.665584Z","iopub.execute_input":"2024-07-10T05:53:47.665983Z","iopub.status.idle":"2024-07-10T05:53:55.006094Z","shell.execute_reply.started":"2024-07-10T05:53:47.665944Z","shell.execute_reply":"2024-07-10T05:53:55.004718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2 Multivariate heatmap","metadata":{}},{"cell_type":"code","source":"# Convert categorical columns to numerical using one-hot encoding\ndf_encoded = pd.get_dummies(df, columns=['Soil_Type', 'Water_Frequency', 'Fertilizer_Type'])\n\n# Compute the correlation matrix\ncorr_matrix = df_encoded.corr()\n\n# Set the size of the plot\nplt.figure(figsize=(12, 10))\n\n# Plot the heatmap\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n\n# Add title to the heatmap\nplt.title('Multivariate Correlation Heatmap of Plant Growth Data')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T05:53:55.008045Z","iopub.execute_input":"2024-07-10T05:53:55.008462Z","iopub.status.idle":"2024-07-10T05:53:55.940285Z","shell.execute_reply.started":"2024-07-10T05:53:55.008424Z","shell.execute_reply":"2024-07-10T05:53:55.939011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 7 : Split the data for Machine learning","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Define features and target","metadata":{}},{"cell_type":"code","source":"X = df_encoded.drop('Growth_Milestone',axis = 1)\ny = df_encoded['Growth_Milestone']","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:10:55.071104Z","iopub.execute_input":"2024-07-10T06:10:55.07151Z","iopub.status.idle":"2024-07-10T06:10:55.078286Z","shell.execute_reply.started":"2024-07-10T06:10:55.071478Z","shell.execute_reply":"2024-07-10T06:10:55.076989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.2 Split the data into training and testing sets","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:12:55.655612Z","iopub.execute_input":"2024-07-10T06:12:55.656062Z","iopub.status.idle":"2024-07-10T06:12:55.665248Z","shell.execute_reply.started":"2024-07-10T06:12:55.656026Z","shell.execute_reply":"2024-07-10T06:12:55.663621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### 7.2 Standardize the feature variables","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T06:12:57.650148Z","iopub.execute_input":"2024-07-10T06:12:57.651325Z","iopub.status.idle":"2024-07-10T06:12:57.665251Z","shell.execute_reply.started":"2024-07-10T06:12:57.651283Z","shell.execute_reply":"2024-07-10T06:12:57.663845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 8 : Train and evaluate a machine learning model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}