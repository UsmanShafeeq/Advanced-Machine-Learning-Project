{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+uf3fj37zbIVp6tl+S7XE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UsmanShafeeq/Advanced-Machine-Learning-Project/blob/main/LLM_END_TO_END_PROJECTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ STEP 1: INSTALL DEPENDENCIES"
      ],
      "metadata": {
        "id": "7SHDJYekT6Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core LLM libraries\n",
        "!pip install -q transformers accelerate sentencepiece bitsandbytes\n"
      ],
      "metadata": {
        "id": "BwGkjy4pT8sz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why these?\n",
        "\n",
        "transformers â†’ load LLMs\n",
        "\n",
        "accelerate â†’ efficient device handling\n",
        "\n",
        "bitsandbytes â†’ memory optimization\n",
        "\n",
        "sentencepiece â†’ tokenizer support"
      ],
      "metadata": {
        "id": "kxOTB311UJ10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: VERIFY HARDWARE (CRITICAL)"
      ],
      "metadata": {
        "id": "TlzYBSaPUSWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Running on CPU\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xQsIXS3ULbM",
        "outputId": "42a85ef8-3e1b-40d6-ad42-e58deaa2418b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: False\n",
            "Running on CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3: SELECT MODEL (OPEN & FREE)"
      ],
      "metadata": {
        "id": "QEkZy3gKUf0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why TinyLLaMA?\n",
        "\n",
        "No login\n",
        "\n",
        "No license issues\n",
        "\n",
        "Perfect for learning\n",
        "\n",
        "Fast on Colab"
      ],
      "metadata": {
        "id": "bFWoIrM3UhTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n"
      ],
      "metadata": {
        "id": "UyBMx80BUar1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4: LOAD TOKENIZER (TEXT â†’ TOKENS)"
      ],
      "metadata": {
        "id": "HCaU-mwnUvAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Tokenizer converts text to numbers (tokens)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(\"Tokenizer loaded successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v8n89gOUwAF",
        "outputId": "a0f25cdf-bdce-4b25-e0f7-7191cee1b9ac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5: LOAD MODEL (LLM BRAIN)"
      ],
      "metadata": {
        "id": "WSur1F4uU5R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",  # Auto CPU/GPU selection\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47rEnSaMU-Rf",
        "outputId": "59d61a2c-6d1e-4c96-aaec-ee68aa6d9bee"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 6: DEVICE MANAGEMENT (BUG-FREE)"
      ],
      "metadata": {
        "id": "RlFDM68yVJ8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-vNTBykVKnJ",
        "outputId": "36e8b39e-4065-4ef3-8d26-0601fb8a998a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 7: PROMPT ENGINEERING (VERY IMPORTANT)\n",
        "Professional Prompt Template"
      ],
      "metadata": {
        "id": "Ar7om64sVRO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(user_input):\n",
        "    \"\"\"\n",
        "    This function standardizes how the AI behaves.\n",
        "    Prompt Engineering happens here.\n",
        "    \"\"\"\n",
        "    system_prompt = (\n",
        "        \"You are a senior AI expert and technology advisor. \"\n",
        "        \"Explain concepts clearly, step by step, with real-world examples.\"\n",
        "    )\n",
        "\n",
        "    return f\"{system_prompt}\\n\\nUser: {user_input}\\nAssistant:\"\n"
      ],
      "metadata": {
        "id": "5FfYMEHfVSSX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEP 8: CHAT FUNCTION (CORE LOGIC)"
      ],
      "metadata": {
        "id": "F5uXQQUjVhW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(user_input):\n",
        "    \"\"\"\n",
        "    Main chatbot function\n",
        "    \"\"\"\n",
        "    prompt = build_prompt(user_input)\n",
        "\n",
        "    # Convert text â†’ tokens\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate AI response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=250,\n",
        "        temperature=0.7,      # creativity control\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    # Convert tokens â†’ text\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "XHAITyTeVl9S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 9: TEST YOUR AI (SANITY CHECK)"
      ],
      "metadata": {
        "id": "MJn2TlaYVtiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat(\"Explain Artificial Intelligence in simple words\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgLSJgSTVxZS",
        "outputId": "f54f7981-f73c-474e-dc87-f451ab88a114"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a senior AI expert and technology advisor. Explain concepts clearly, step by step, with real-world examples.\n",
            "\n",
            "User: Explain Artificial Intelligence in simple words\n",
            "Assistant: Artificial Intelligence (AI) is a branch of computer science that enables machines to perform tasks that were previously thought to require human intelligence. AI can be used in various industries, including healthcare, finance, and education.\n",
            "\n",
            "AI in healthcare:\n",
            "\n",
            "1. Medical Imaging: AI is used for image analysis in medical imaging, such as CT scans, MRI, and X-rays. It can identify abnormalities in the images and diagnose diseases more accurately.\n",
            "\n",
            "2. Drug Discovery: AI is used to predict drug interactions and optimize drug therapy. It can also identify new drug targets and predict drug efficacy.\n",
            "\n",
            "3. Medical Devices: AI is used for diagnosing diseases, monitoring patient health, and predicting medical complications.\n",
            "\n",
            "4. Personalized Medicine: AI can analyze genetic data to identify patients at higher risk of developing diseases. It can also recommend personalized treatments.\n",
            "\n",
            "5. Virtual and Augmented Reality: AI is used to create virtual reality experiences, including medical training and therapy.\n",
            "\n",
            "AI in finance:\n",
            "\n",
            "1. Fra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 10: TERMINAL-STYLE CHATBOT (REAL FEEL)"
      ],
      "metadata": {
        "id": "nwK9uzo0V1Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AI Chatbot Started (type 'exit' to stop)\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Session ended.\")\n",
        "        break\n",
        "\n",
        "    reply = chat(user_input)\n",
        "    print(\"\\nAI:\", reply)\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DP_HsXSnV-Kq",
        "outputId": "e361fc3c-98b7-4e8e-f417-5dac4f79366d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Chatbot Started (type 'exit' to stop)\n",
            "\n",
            "You: Explain Artificial Intelligence in simple words\n",
            "\n",
            "AI: You are a senior AI expert and technology advisor. Explain concepts clearly, step by step, with real-world examples.\n",
            "\n",
            "User: Explain Artificial Intelligence in simple words\n",
            "Assistant: Artificial Intelligence (AI) is a branch of computer science that enables machines to perform tasks that were previously only performed by humans. AI is a subset of computer science that deals with creating intelligent machines, computers, and software that can perform tasks that are difficult or impossible for humans to perform.\n",
            "\n",
            "AI can be divided into two main categories:\n",
            "\n",
            "1. Machine Learning: It is a subset of AI that allows machines to learn and adapt to new situations. It uses algorithms to analyze data, identify patterns, and make predictions based on this data.\n",
            "\n",
            "2. Deep Learning: It is a subset of Machine Learning that allows machines to learn by themselves. It uses neural networks to process data, making it possible to perform tasks that were previously impossible for humans.\n",
            "\n",
            "Real-world examples of AI:\n",
            "\n",
            "1. Amazon: Amazon uses machine learning to recommend products to customers based on their previous purchases.\n",
            "\n",
            "2. Google: Google uses machine learning to search for images, videos, and news articles.\n",
            "\n",
            "3. Netflix: Netflix uses machine learning to personalize the content that users watch.\n",
            "\n",
            "4. Uber: Uber uses machine learning to predict\n",
            "--------------------------------------------------\n",
            "You: stop\n",
            "\n",
            "AI: You are a senior AI expert and technology advisor. Explain concepts clearly, step by step, with real-world examples.\n",
            "\n",
            "User: stop\n",
            "Assistant: Of course. Here's a breakdown of some key concepts in AI and their applications in the real world:\n",
            "\n",
            "1. Artificial Intelligence: AI is the ability of machines to simulate human intelligence. It can perform tasks that require human intelligence, such as problem-solving, decision-making, and language processing. AI technologies are being used in various industries, including healthcare, finance, and transportation.\n",
            "\n",
            "2. Machine Learning: Machine learning is the process of teaching a machine to learn from data without being explicitly programmed. It's used in applications like predictive maintenance, fraud detection, and customer service.\n",
            "\n",
            "3. Deep Learning: Deep learning is a type of AI that uses neural networks to learn from data. It's used in applications like image recognition, natural language processing, and speech recognition.\n",
            "\n",
            "4. Natural Language Processing (NLP): NLP is the process of analyzing and understanding human language. It's used in applications like chatbots, virtual assistants, and language translation.\n",
            "\n",
            "5. Robotics: Robotics is the study and development of robots, including autonomous systems and hum\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3587529301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 11: STREAMING RESPONSE (ADVANCED UX)"
      ],
      "metadata": {
        "id": "a69MzWePWQLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def stream_chat(user_input):\n",
        "    prompt = build_prompt(user_input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    for word in text.split():\n",
        "        print(word, end=\" \", flush=True)\n",
        "        time.sleep(0.03)\n"
      ],
      "metadata": {
        "id": "7mfNvpGoWQ_L"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream_chat(\"Explain LLMs like a CTO\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7aU9o5MWXLT",
        "outputId": "cdd01bb9-2e7a-44e4-ea72-ee8851c02b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cq1bqbTmWUbE"
      }
    }
  ]
}